import streamlit as st
from openai import OpenAI
import requests
import json
import re
from duckduckgo_search import DDGS

# from fpdf import FPDF
from langchain_openai import ChatOpenAI
import os

# å¯¼å…¥ LangChain æ¶ˆæ¯å¯¹è±¡
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_community.utilities import GoogleSerperAPIWrapper

os.environ["SERPER_API_KEY"] = "9fd7b3cb044ed5a235e8a14a3c72e3e8b7dd2cbc"
os.environ["HTTPS_PROXY"] = "http://127.0.0.1:7890"
os.environ["HTTPS_PROXY"] = "http://127.0.0.1:7890"
# ========== åŸºç¡€é…ç½® ==========
st.set_page_config(page_title="AI å¸‚åœºè°ƒç ”åŠ©ç†", layout="wide")
st.title("ğŸ§  AI å¸‚åœºè°ƒç ”åŠ©ç† ")
st.markdown("è®© AI å¸®ä½ å¿«é€Ÿå®Œæˆè¡Œä¸šè¶‹åŠ¿ã€ç«äº‰æ ¼å±€ã€æ¶ˆè´¹è€…æ´å¯Ÿç­‰å¸‚åœºè°ƒç ”ä»»åŠ¡ã€‚")
base_url = "https://yinli.one/v1"
# api_key = ("sk-LigUlIOoxblNRsIW83Ivom303rVkgteWazFVDe4JldylDkPU",)
# ========== è¾“å…¥åŒº ==========
# ã€é‡è¦ä¿®æ”¹ 2ï¼šæç¤ºç”¨æˆ·è¾“å…¥ API Keyã€‘
api_key = st.text_input("è¯·è¾“å…¥ä½ çš„ API Keyï¼š", type="password")
query = st.text_input("è¯·è¾“å…¥è°ƒç ”ä¸»é¢˜ï¼ˆä¾‹å¦‚ï¼šæ–°èƒ½æºè½¦å¸‚åœºè¶‹åŠ¿ï¼‰")
generate_btn = st.button("ğŸš€ å¼€å§‹ç”ŸæˆæŠ¥å‘Š")


# ========== åŠŸèƒ½å‡½æ•° (ä¿æŒä¸å˜) ==========
def search_market_info(query: str):
    """ä½¿ç”¨ DuckDuckGo å…è´¹æ¥å£è¿›è¡Œä¿¡æ¯æ£€ç´¢"""
    st.info("ğŸ” æ­£åœ¨æœç´¢å¸‚åœºæ•°æ®...")
    try:
        # æ³¨æ„ï¼šddg-api.herokuapp.com æ¥å£å¯èƒ½ä¸ç¨³å®šï¼Œä½†æ­¤å¤„æ²¿ç”¨åŸä»£ç 
        res = requests.get(
            f"https://ddg-api.herokuapp.com/search?q={query}&max_results=8"
        )
        data = res.json()
        if isinstance(data, list):
            merged_text = "\n\n".join([f"â€¢ {r['title']}\n{r['snippet']}" for r in data])
            return merged_text
        return "æœªæ‰¾åˆ°ç›¸å…³ç»“æœã€‚"
    except Exception as e:
        st.error(f"æ£€ç´¢å‡ºé”™ï¼š{e}")
        return "æ£€ç´¢å‡ºé”™ï¼Œæ— æ³•è·å–å¤–éƒ¨æ•°æ®ã€‚"


def save_as_pdf(text, filename="market_report.pdf"):
    """å°†æŠ¥å‘Šå†…å®¹å¯¼å‡ºä¸º PDF"""
    pdf = FPDF()
    pdf.add_page()
    pdf.set_font("Arial", size=12)
    for line in text.split("\n"):
        pdf.multi_cell(0, 8, line)
    pdf.output(filename)
    return filename


def clean_and_load_json(text_output: str):
    """å°è¯•ä» LLM çš„è¾“å‡ºä¸­æå–å¹¶åŠ è½½ JSON"""
    text_output = re.sub(r"[\u200b-\u200f\uFEFF\xa0]", "", text_output)
    # Step 1: å°è¯•åŒ¹é…```json ... ```ä»£ç å—ä¸­çš„å†…å®¹
    match = re.search(
        r"```(?:json)?\s*(\{[\s\S]*?\})\s*```",
        text_output,
        re.DOTALL | re.IGNORECASE,  # DOTALL è®© . åŒ¹é…æ¢è¡Œç¬¦
    )

    if not match:
        match = re.search(r"(\{[\s\S]*?\})", text_output.strip())
        if not match:
            raise ValueError("æœªèƒ½æ‰¾åˆ°æœ‰æ•ˆçš„ JSON ä»£ç å—æˆ–è£¸ JSON ç»“æ„ã€‚")

    # æå–ä»£ç å—ä¸­çš„çº¯ JSON å­—ç¬¦ä¸²
    json_string = match.group(1).strip()

    # æ­¥éª¤ 2: è¿›è¡Œ JSON è§£æ
    # æ³¨æ„ï¼šæˆ‘ä»¬å‡è®¾æå–å‡ºçš„ json_string æ˜¯å¹²å‡€çš„
    return json.loads(json_string)


def search_ddg(query, max_results=5):
    try:
        results = DDGS().text(query, max_results=max_results)
        texts = [r["body"] for r in results]
        return "\n".join(texts)
    except Exception as e:
        return f"DDG æœç´¢å¤±è´¥ï¼š{e}"


# ========== ä¸»æµç¨‹ ==========
if generate_btn:
    if not api_key or not query:
        st.warning("âš ï¸ è¯·è¾“å…¥ API Key å’Œ è°ƒç ”ä¸»é¢˜")
        st.stop()

        # åˆ›å»º LangChain Chat æ¨¡å‹å®ä¾‹ï¼ˆåŸºçº¿æ¸©åº¦ä¸º 0ï¼Œå¯åœ¨ç”ŸæˆæŠ¥å‘Šæ—¶è°ƒæ•´ï¼‰
    llm = ChatOpenAI(
        api_key=api_key,
        base_url=base_url,
        temperature=0.2,  # éšæœºæ€§ï¼š0ï¼ˆç¡®å®šæ€§ï¼‰~1ï¼ˆåˆ›é€ æ€§ï¼‰
        model="gpt-4o-mini",
    )

    # Step 1ï¸âƒ£ è°ƒç ”æ–¹å‘è¯†åˆ«
    with st.spinner("ğŸ§  æ­£åœ¨è¯†åˆ«è°ƒç ”æ–¹å‘..."):
        # å®Œæ•´çš„ System Prompt
        system_prompt_content = """
        # è§’è‰²
        ä½ æ˜¯ä¸€ä½ä¸“ä¸šä¸”ç»éªŒæ·±åšçš„å¸‚åœºåˆ†æä¸“å®¶ï¼Œå‡­å€Ÿæ‰å®çš„ä¸“ä¸šçŸ¥è¯†å’Œä¸°å¯Œçš„å®æˆ˜ç»éªŒï¼Œä¸ºç”¨æˆ·æ·±å…¥å‰–æå„ç±»å¸‚åœºé—®é¢˜ã€‚
        
        ## æŠ€èƒ½
        ### æŠ€èƒ½ 1: åˆ¤æ–­é—®é¢˜ç±»å‹
        ä»”ç»†ç ”è¯»ç”¨æˆ·è¾“å…¥ï¼Œç²¾ç¡®åˆ¤æ–­é—®é¢˜æ‰€å±ç±»å‹ï¼Œä¸ºç”¨æˆ·åŒ¹é…æœ€åˆé€‚çš„æ–¹å‘ç±»åˆ«ï¼Œç±»å‹æ¶µç›–è¡Œä¸šè¶‹åŠ¿ã€ç«å“åˆ†æã€ç”¨æˆ·ç”»åƒã€æŠ•èµ„åˆ†æå•†ä¸šæ¨¡å¼ç­‰å„ç±»å¸‚åœºåˆ†æç›¸å…³é¢†åŸŸã€‚
        
        ### 2: æ•°æ®ä¿¡æ¯å…³è”
        ä¾æ®åˆ¤æ–­å‡ºçš„é—®é¢˜ç±»å‹ï¼Œå…³è”ä¸ä¹‹åŒ¹é…çš„å„ç±»å¸‚åœºæ•°æ®ä¿¡æ¯ï¼Œä¸ºåç»­åˆ†ææä¾›æœ‰åŠ›æ”¯æ’‘ã€‚
        
        ### æŠ€èƒ½ 3: æå–å…³é”®è¯
        ä»ç”¨æˆ·è¾“å…¥ä¸­ç²¾å‡†æç‚¼å…³é”®ä¿¡æ¯ï¼Œå…³é”®è¯åŒ…å«ä½†ä¸é™äºè¡Œä¸šã€åœ°åŒºã€æ—¶é—´ã€å“ç‰Œã€äº§å“ç­‰ã€‚è‹¥ç”¨æˆ·æœªæä¾›æ—¶é—´ä¿¡æ¯ï¼Œé»˜è®¤æ—¶é—´ä¸ºæœ€è¿‘ä¸€å¹´ï¼Œå³ 2025 å¹´ï¼›è‹¥æœªæåŠåœ°åŒºï¼Œé»˜è®¤åœ°åŒºä¸ºä¸­å›½ã€‚
        
        ### æŠ€èƒ½ 4: æ‰©å†™æœç´¢æŸ¥è¯¢
        å›´ç»•æå–çš„å…³é”®è¯ï¼Œç²¾å¿ƒæ‰©å†™å‡º 4 ä¸ªè¯­ä¹‰ç›¸è¿‘ä½†æè¿°æ›´ä¸ºè¯¦å°½çš„æœç´¢æŸ¥è¯¢ã€‚è¿™äº›æŸ¥è¯¢éœ€å……åˆ†èåˆç”¨æˆ·åŸå§‹é—®é¢˜ä»¥åŠå…³é”®è¯çš„ç›¸å…³ä¿¡æ¯ï¼Œç¡®ä¿æŸ¥è¯¢æ›´å…·é’ˆå¯¹æ€§ä¸å…¨é¢æ€§ã€‚
        
        ## è¾“å‡ºæ ¼å¼
        ä»¥ json æ ¼å¼è¾“å‡ºç»“æœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
        ```json{
        Â  "intent": "é—®é¢˜ç±»å‹",Â Â 
        Â  "entities": {
        Â  Â  "industry": "è¡Œä¸šåç§°",
        Â  Â  "region": "åœ°åŒºåç§°",
        Â  Â  "time_range": "æ—¶é—´èŒƒå›´"
        Â  },
        Â  "expanded_queries": [
        Â  Â  "æœç´¢æŸ¥è¯¢ 1",
        Â  Â  "æœç´¢æŸ¥è¯¢ 2",
        Â  Â  "æœç´¢æŸ¥è¯¢ 3",
        Â  Â  "æœç´¢æŸ¥è¯¢ 4"
        Â  ]
        }```
        
        ## é™åˆ¶
        - ä»…å¤„ç†å’Œå›ç­”ä¸å¸‚åœºåˆ†æç´§å¯†ç›¸å…³çš„ç”¨æˆ·é—®é¢˜ï¼Œåšå†³æ‹’ç»å›ç­”æ— å…³è¯é¢˜ã€‚
        - è¾“å‡ºå†…å®¹å¿…é¡»ä¸¥æ ¼éµå¾ªç»™å®šçš„ json æ ¼å¼è¿›è¡Œç»„ç»‡ï¼Œä¸å¾—å‡ºç°ä»»ä½•æ ¼å¼åå·®ã€‚
        - è¯·ç¡®ä¿è¾“å‡ºçš„æ ¼å¼æ˜¯json
        """

        # å®šä¹‰ LangChain æ¶ˆæ¯åˆ—è¡¨
        intent_messages = [
            SystemMessage(content=system_prompt_content),
            HumanMessage(content=f"è¯·åˆ†æè¿™ä¸ªä¸»é¢˜ï¼š{query}"),
        ]

        # ä½¿ç”¨ LangChain çš„ .invoke() æ–¹æ³•è¿›è¡Œè°ƒç”¨
        intent_resp = llm.invoke(intent_messages)
        intent_text = intent_resp.content.strip()

        try:
            intent_json = clean_and_load_json(intent_text)
            if len(intent_json.get("expanded_queries", [])) < 2:
                raise ValueError("expanded_queries æ•°é‡ä¸è¶³")
            # ä» expanded_queries ä¸­è·å–ç¬¬ä¸€ä¸ªæŸ¥è¯¢ç”¨äº Step 2 çš„æœç´¢
            # search_query_for_ddg = intent_json["expanded_queries"][0]
        except Exception as e:
            st.error(f"âš ï¸ JSONè§£æå¤±è´¥ï¼Œæ¨¡å‹æœªè¿”å›æ­£ç¡®çš„ç»“æ„åŒ–æ•°æ®ã€‚é”™è¯¯ï¼š{e}")
            # å¤±è´¥æ—¶ä½¿ç”¨é»˜è®¤å€¼
            intent_json = {
                "intent": "è¡Œä¸šè¶‹åŠ¿",
                "entities": {"industry": query, "region": "ä¸­å›½", "time_range": "2025"},
                "expanded_queries": [f"{query}å¸‚åœºè§„æ¨¡åˆ†æ", f"{query}æœ€æ–°å‘å±•æƒ…å†µ"],
            }

            st.session_state.intent_json = intent_json
            st.session_state.confirmed = False  # åˆå§‹åŒ–ç¡®è®¤çŠ¶æ€
        if "final_queries" not in st.session_state:
            st.session_state.final_queries = intent_json["expanded_queries"]
            st.divider()

            # Step 2ï¸âƒ£ ç”¨æˆ·ç¡®è®¤/ä¿®æ”¹æœç´¢å…³é”®è¯
            st.subheader("ğŸ” æœç´¢å…³é”®è¯ç¡®è®¤ä¸ä¿®æ”¹")
            st.markdown(
                "æ¨¡å‹å·²ä¸ºæ‚¨ç”Ÿæˆä»¥ä¸‹æœç´¢å…³é”®è¯ã€‚æ‚¨å¯ä»¥ç›´æ¥**ç¡®è®¤**è¿›è¡Œæœç´¢ï¼Œæˆ–åœ¨æ–‡æœ¬æ¡†ä¸­**ä¿®æ”¹**åç‚¹å‡»**é‡æ–°ç”Ÿæˆ**ã€‚"
            )
            # 1. å±•ç¤ºå’Œç¼–è¾‘åŒºåŸŸ
            # å°†åˆ—è¡¨è½¬æ¢æˆå¸¦ç¼–å·çš„å­—ç¬¦ä¸²ï¼Œæ–¹ä¾¿ç”¨æˆ·ç¼–è¾‘
            queries_text = "\n".join(st.session_state.final_queries)
            # ä½¿ç”¨ text_area å…è®¸ç”¨æˆ·ç¼–è¾‘ï¼Œå¹¶å­˜å‚¨åœ¨ session state çš„ä¸´æ—¶å˜é‡ä¸­
            edited_queries_text = st.text_area(
                "ğŸ“ è¯·ç¡®è®¤æˆ–ä¿®æ”¹å…³é”®è¯",
                value=queries_text,
                height=200,
                key="edited_queries_text",  # ç¡®ä¿ Streamlit èƒ½å¤Ÿè·Ÿè¸ªçŠ¶æ€
            )

            # ç”¨æˆ·ç¡®è®¤æŒ‰é’®
            col1, col2 = st.columns(2)
            with col1:
                confirm_btn = st.button(
                    "ğŸš€ ç¡®è®¤æ— è¯¯ï¼Œå¼€å§‹æœç´¢", use_container_width=True
                )
                if confirm_btn:
                    st.session_state.confirmed = True

            with col2:
                if st.button("âœï¸ ä¿®æ”¹è°ƒç ”æ–¹å‘"):
                    del st.session_state.intent_json
                    st.session_state.confirmed = False

            search_query_for_ddg = query

        st.subheader("ğŸ“˜ è°ƒç ”æ–¹å‘è¯†åˆ«ç»“æœ")
        st.json(intent_json)

    # # Step 2ï¸âƒ£ æ£€ç´¢å¸‚åœºä¿¡æ¯
    with st.spinner("ğŸ” æ­£åœ¨æ£€ç´¢å¸‚åœºæ•°æ®..."):
        search_wrapper = GoogleSerperAPIWrapper()
        # è°ƒç”¨ results() æ–¹æ³•è·å–åŸå§‹ç»“æ„åŒ–æ•°æ®
        raw_data = search_wrapper.results(query, num=5)  # æ˜ç¡®æŒ‡å®š num=5
        # æå–å‰ N æ¡ç»“æœï¼ˆä¾‹å¦‚å‰ 3 æ¡ï¼‰
        num_results_needed = 3
        if "organic" in raw_data:
            # Serper çš„ä¸»è¦æœç´¢ç»“æœåœ¨ 'organic' é”®ä¸­
            top_results = raw_data["organic"][:num_results_needed]

            print(f"âœ… æˆåŠŸè·å– {len(top_results)} æ¡ç»“æ„åŒ–æœç´¢ç»“æœï¼š{top_results}")
    for i, result in enumerate(top_results):
        print(f"--- ç»“æœ {i+1} ---")
        print(f"æ ‡é¢˜: {result.get('title')}")
        print(f"æ‘˜è¦: {result.get('snippet')[:100]}...")  # æ‰“å°æ‘˜è¦å‰100å­—ç¬¦
        print(f"é“¾æ¥: {result.get('link')}")
    else:
        print("âŒ æœç´¢å¤±è´¥æˆ–ç»“æœä¸ºç©ºã€‚")
#     st.success("æ•°æ®æ£€ç´¢å®Œæˆã€‚")

# Step 3ï¸âƒ£ ç”ŸæˆæŠ¥å‘Š
# with st.spinner("ğŸ§¾ æ­£åœ¨ç”Ÿæˆå¸‚åœºæŠ¥å‘Š..."):

#     report_prompt = f"""
#     ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¸‚åœºåˆ†æé¡¾é—®ï¼Œè¯·æ ¹æ®ä»¥ä¸‹èµ„æ–™ï¼Œä¸ºä¸»é¢˜â€œ{query}â€ç”Ÿæˆä¸€ä»½ç»“æ„åŒ–å¸‚åœºè°ƒç ”æŠ¥å‘Šï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
#     ---
#     ## è¡Œä¸šæ¦‚è¿°
#     ...
#     ## ä¸»è¦è¶‹åŠ¿
#     ...
#     ## ç«äº‰æ ¼å±€
#     ...
#     ## ç”¨æˆ·æ´å¯Ÿ
#     ...
#     ## æ€»ç»“ä¸å»ºè®®
#     ...
#     ---
#     ä»¥ä¸‹æ˜¯å‚è€ƒèµ„æ–™ï¼š
#     {merged_info}
#     """

#     # å®šä¹‰ç”ŸæˆæŠ¥å‘Šçš„æ¶ˆæ¯åˆ—è¡¨
#     report_messages = [
#         SystemMessage(
#             content="ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¸‚åœºåˆ†æé¡¾é—®ï¼Œä¸“æ³¨äºç”Ÿæˆç»“æ„ä¸¥è°¨ã€å†…å®¹æ·±å…¥çš„å¸‚åœºè°ƒç ”æŠ¥å‘Šã€‚"
#         ),
#         HumanMessage(content=report_prompt),
#     ]

#     # è°ƒæ•´æ¸©åº¦ä»¥è·å¾—æ›´å…·åˆ›é€ æ€§çš„æŠ¥å‘Š
#     llm.temperature = 0.5
#     report_resp = llm.invoke(report_messages)
#     report_text = report_resp.content.strip()

#     st.success("âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
#     st.subheader("ğŸ“„ å¸‚åœºè°ƒç ”æŠ¥å‘Š")
#     st.markdown(report_text)

#     # Step 4ï¸âƒ£ å¯¼å‡ºåŠŸèƒ½
#     st.download_button(
#         label="ğŸ’¾ ä¸‹è½½æŠ¥å‘Šï¼ˆMarkdownï¼‰",
#         data=report_text,
#         file_name="market_report.md",
#         mime="text/markdown",
#     )

#     # å¯¼å‡º PDF
#     pdf_path = save_as_pdf(report_text)
#     with open(pdf_path, "rb") as f:
#         st.download_button(
#             label="ğŸ“„ ä¸‹è½½æŠ¥å‘Šï¼ˆPDFï¼‰",
#             data=f,
#             file_name="market_report.pdf",
#             mime="application/pdf",
#         )
